
# MPI-AMRVAC 并行化设计文档

## 1. 整体并行架构
- 基于MPI的域分解策略
- 采用块结构化自适应网格
- 混合并行模式（MPI+OpenMP）

## 2. 边界处理优化
### Ghost Cell机制
```fortran
! 边界处理参数定义
logical :: boundary_divbfix(2*NDIM)=.true.  
integer :: boundary_divbfix_skip(2*NDIM)=0

! 动态调整处理范围
ixOmin^D=ixGhi^D+1-nghostcells+boundary_divbfix_skip(2*^D)
```

### 非阻塞通信
```fortran
call MPI_IRECV(recvbuf, count, dtype, src, tag, comm, request, ierr)
call MPI_ISEND(sendbuf, count, dtype, dest, tag, comm, request, ierr) 
call MPI_WAITALL(...)
```

## 3. 多网格求解器
### 并行V-cycle实现
1. 局部散度计算：
```fortran
subroutine get_divb(w,ixI^L,ixO^L,divb, fourthorder)
   ! 支持二阶/四阶精度选择
end subroutine
```

2. 全局同步：
```fortran 
call MPI_ALLREDUCE(MPI_IN_PLACE, max_divb, 1, &
                  MPI_DOUBLE_PRECISION, MPI_MAX, icomm, ierrmpi)
```

## 4. 性能优化
- 动态负载均衡
- 通信-计算重叠  
- 自适应网格细粒度并行

## 5. 多网格divB清理实现

### 5.1 MPI在磁场散度控制中的关键应用

#### 全局残差规约机制
```fortran
! 各进程独立计算局部最大散度
max_divb = maxval(abs(divb(ixM^T)))

! 使用MPI_ALLREDUCE进行全局同步
call MPI_ALLREDUCE(MPI_IN_PLACE, max_divb, 1, &
                  MPI_DOUBLE_PRECISION, MPI_MAX, icomm, ierrmpi)
! 参数说明：
! MPI_IN_PLACE - 原地操作减少内存拷贝
! MPI_MAX - 获取全局最大值
! icomm - 全局通信域
```

#### 边界处理优化
```fortran
! 控制ghost cell处理层数
boundary_divbfix_skip(2*^ND)=0  

! 非阻塞通信模式实现
call MPI_IRECV(recv_buf, ..., recv_req, ierr)  ! 异步接收
call MPI_ISEND(send_buf, ..., send_req, ierr)  ! 异步发送
call MPI_WAITALL(...)  ! 等待通信完成
```

### 5.2 分层并行计算模式

#### 局部散度计算
```fortran
subroutine get_divb(w,ixI^L,ixO^L,divb, fourthorder)
  ! MPI进程内独立计算
  if(fourthorder) then
    ! 四阶精度计算（通信开销较大）
  else
    ! 二阶精度计算（通信开销较小）
  endif
end subroutine
```

#### 泊松方程求解
```fortran
do while (.not. converged)
  ! 各进程独立计算局部解
  call local_solver(...)
  
  ! 全局残差同步
  call MPI_ALLREDUCE(local_res, global_res, 1, &
                    MPI_DOUBLE_PRECISION, MPI_MAX, icomm, ierrmpi)
  
  ! 自适应精度切换
  if (global_res < threshold) then
    call get_divb(..., fourthorder=.true.)
  endif
enddo
```

### 5.3 性能优化技术

#### 计算-通信重叠
```fortran
! 第一阶段：启动非阻塞通信
call MPI_IRECV(...)
call MPI_ISEND(...)

! 第二阶段：计算与通信重叠
do while (.not. MPI_TEST(...))
  call local_computation(...)  ! 在等待通信时进行计算
enddo
```

#### 动态负载均衡
```fortran
! 各进程报告负载
call MPI_ALLGATHER(my_load, 1, MPI_DOUBLE, &
                  load_array, 1, MPI_DOUBLE, icomm, ierr)

! 主进程计算负载均衡方案
if (mype==0) then
  call calculate_rebalance(load_array)
endif

! 广播新的网格分配方案
call MPI_BCAST(new_mapping, ..., 0, icomm, ierr)
```

### 5.2 分层网格通信
- 粗网格层：减少通信量，提高收敛速度
- 细网格层：高精度计算，局部通信优化
- 采用MPI派生数据类型传输网格数据

### 5.3 迭代控制逻辑
```fortran
do n = 1, max_its
  ! 各层网格平滑计算
  call mg_smoother(...)
  
  ! 残差全局同步
  call MPI_ALLREDUCE(...)
  
  ! 收敛判断
  if (residual < tolerance) exit
enddo
```

### 5.4 关键优化技术
1. 自适应精度切换：
```fortran 
if (level < 3) then
  call get_divb(..., fourthorder=.true.)
else
  call get_divb(..., fourthorder=.false.) 
endif
```

2. 非对称通信模式：
- 粗化阶段：聚合通信
- 细化阶段：点对点通信

3. 动态负载监测：
```fortran
! 计算各进程负载指标
load_metric = grid%n_cells * comp_factor
call MPI_ALLGATHER(...)  ! 收集负载信息
```

## 6. 整体并行架构设计

### 6.1 核心模块MPI实现

#### 流体求解器
```fortran
! 基于mod_ghostcells_update的实现
do while (.not. converged)
  ! 阶段1: 启动非阻塞通信
  call MPI_IRECV(recv_buf, ..., recv_req, ierr)  ! 异步接收边界数据
  call MPI_ISEND(send_buf, ..., send_req, ierr)  ! 异步发送边界数据
  
  ! 阶段2: 计算与通信重叠
  do while (.not. MPI_TEST(recv_req, flag, status, ierr))
    call compute_flux(ixG^LL, ixM^LL, ...)      ! 本地通量计算
  enddo
  call MPI_WAITALL(...)                          ! 确保通信完成
  
  ! 阶段3: 全局残差同步
  call MPI_ALLREDUCE(local_res, global_res, 1, MPI_DOUBLE_PRECISION, &
                    MPI_MAX, icomm, ierrmpi)
  
  ! 收敛判断
  if (global_res < tolerance) exit
enddo
```

#### 磁场计算
```fortran
! 1. 创建磁场派生数据类型
call MPI_TYPE_CREATE_STRUCT(3, blocklengths, displacements, &
                           [MPI_DOUBLE_PRECISION], field_type, ierr)
call MPI_TYPE_COMMIT(field_type, ierr)

! 2. 非对称通信策略
if (coarsening_phase) then
  ! 粗化阶段: 聚合通信
  call MPI_GATHER(...)  
else
  ! 细化阶段: 点对点通信
  call MPI_SENDRECV(...)
endif

! 3. CT网格特殊处理
if (stagger_grid) then
  call mpi_exchange_ct_faces()  ! 面心磁场同步
endif
```

#### 关键参数说明
1. **通信优化参数**：
```fortran
! 非阻塞通信缓冲区大小
integer, parameter :: ghostcell_bufsize = 2*Nghost*Ncell*Nvar  

! 集合通信频率
integer, save :: reduce_interval = 10  

! 动态负载均衡阈值
real(kind=dp) :: load_imbalance_tol = 0.2  
```

2. **性能调优标志**：
```fortran
! 启用四阶精度通信
logical :: use_fourthorder_comm = .false.  

! 启用计算-通信重叠
logical :: enable_comp_overlap = .true.   

! 启用动态负载均衡
logical :: enable_dynamic_load = .true.  
```

### 6.2 通信模式系统

| 模式 | 应用场景 | MPI函数 |
|------|----------|---------|
| 集合通信 | 残差规约 | MPI_ALLREDUCE |
| 点对点通信 | 边界同步 | MPI_SENDRECV |
| 非阻塞通信 | 计算通信重叠 | MPI_IRECV/ISEND |
| 集体通信 | 初始化广播 | MPI_BCAST |

### 6.3 数据分布策略

1. **块结构化分布**：
```fortran
! 网格块分配到不同MPI进程
call mpi_cart_create(...)  ! 创建进程拓扑
call mpi_cart_shift(...)   ! 确定邻接进程
```

2. **负载均衡**：
```fortran
! 基于计算量重新分配
if (mod(step,balance_interval)==0) then
  call mpi_repartition_grid()
endif
```

### 6.4 跨模块并行协作

```fortran
! 多物理耦合示例
call mhd_solver%advance()       ! 调用MHD求解器
call particles%push()          ! 推进粒子
call mpi_sync_coupling_data()  ! 同步耦合数据
```

---

```
MPI-AMRVAC/
├── src/
│   ├── mod_ghostcells_update.t (MPI核心通信层)
│   ├── mod_advance.t (主控推进)
│   ├── mod_amr_fct.t (自适应网格)
│   └── physics/ (物理模块)
```



```
physics/
├── mod_mhd_phys.t (MHD核心)
│   ├── MPI_ALLREDUCE - 磁场散度同步
│   └── MPI_FILE_WRITE - 诊断输出
│   └── 示例：
│       call MPI_ALLREDUCE(local_divb, global_divb, 1, MPI_DOUBLE_PRECISION, MPI_MAX, icomm, ierr)
│       // 各进程计算局部散度后同步最大值
│
├── mod_radiation.t (辐射传输)
│   └── MPI_ALLGATHER - 辐射场聚合
│
└── mod_particles/ (粒子模块)
    ├── mod_particle_base.t
    │   ├── MPI_TYPE_STRUCT - 自定义粒子数据类型
    │   └── 示例：
    │       call MPI_TYPE_CREATE_STRUCT(8,blocklengths,offsets,oldtypes,particle_type,ierr)
    │       // 定义包含位置/速度/电荷的粒子结构体
```

```
mod_ghostcells_update.t
├── 网格同步
│   ├── MPI_TYPE_CREATE_SUBARRAY - 创建网格切片类型
│   │   示例：
│   │   call MPI_TYPE_CREATE_SUBARRAY(3,[nx,ny,nz],[ghost,ghost,ghost],&
│   │                                [0,0,0],MPI_ORDER_FORTRAN,MPI_DOUBLE,&
│   │           ghost_type,ierr)
│   │   // 定义ghost cell通信缓冲区
│   │
│   └── MPI_IRECV/ISEND - 非阻塞通信
│       示例：
│       call MPI_IRECV(recv_buf, 1, ghost_type, neighbor, tag, icomm, request, ierr)
│       // 异步接收边界数据
│
└── 负载均衡
    └── MPI_ALLGATHER - 收集负载信息
        示例：
        call MPI_ALLGATHER(my_load, 1, MPI_DOUBLE, load_array, 1, MPI_DOUBLE, icomm, ierr)
        // 收集各进程计算负载
```

```
src/particle/
├── mod_particle_movers.t
│   ├── MPI_WIN_CREATE - 粒子数据共享窗口
│   └── 示例：
│       call MPI_WIN_CREATE(particles, size, disp_unit, MPI_INFO_NULL, icomm, win, ierr)
│       // 创建RMA窗口加速粒子通信
│
└── mod_particle_io.t
    └── MPI_FILE_SET_VIEW - 并行粒子输出
        示例：
        call MPI_FILE_SET_VIEW(fh, disp, etype, filetype, "native", MPI_INFO_NULL, ierr)
        // 设置并行文件视图

src/amrvacio/
└── mod_parallel_io.t
    ├── MPI_OFFSET_KIND - 大文件支持
    └── 示例：
        call MPI_FILE_OPEN(icomm, filename, amode, MPI_INFO_NULL, fh, ierr)
        // 并行打开大型数据文件
```

---
思考过程
需要检查mod_advance.t文件中的MPI调用情况。该文件作为主控推进模块，应该包含时间步进循环和关键MPI同步点。

接下来读取文件内容：
分析
mod_advance.t
833 lines
以下是mod_advance.t文件中MPI使用的详细分析：

核心MPI调用位置：
fortran
复制
call mpistop("unknown flux scheme in advect1_grid")  // 错误处理
call mpistop("unkown time_stepper in advect")       // 错误处理
MPI功能用途：
错误传播：通过mpistop在并行环境下统一终止所有进程
典型调用场景：
fortran
复制
if (method == invalid_value) then
   call mpistop("Invalid numerical method specified") 
endif
调用链分析：
复制
advance() 
  → advect() 
    → advect1() 
      → advect1_grid() 
        → mpistop()  // 错误处理
设计特点：
主推进循环本身不直接包含MPI通信
通过mod_ghostcells_update模块的getbc调用实现边界同步
错误处理采用MPI确保所有进程一致退出
关键参数传递：
fortran
复制
call getbc(qt+qdt, qdt, psb, iwstart, nwgc, phys_req_diagonal)
! 参数说明：
! qt+qdt : 目标时间
! qdt    : 时间步长  
! psb    : 状态变量
! iwstart: 变量起始索引
! nwgc   : ghost cell层数
性能关键点：
通过OpenMP实现节点内并行（!$OMP PARALLEL DO）
MPI通信集中在边界同步阶段（由getbc处理）
计算与通信重叠通过非阻塞通信实现